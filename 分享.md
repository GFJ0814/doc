spark-streaming分享准备

一、sparkUI含义：

对于Spark Streaming来说，以下几个度量指标尤为重要（在Batch Processing Statistics标签下）：

Processing Time：处理每个batch的时间 
Scheduling Delay：每个batch在队列中等待前一个batch完成处理所等待的时间

若Processing Time的值一直大于Scheduling Delay，或者Scheduling Delay的值持续增长，代表系统已经无法处理这样大的数据输入量了，这时就需要考虑各种优化方法来增强系统的负载。

保证每个batch的数据能够在batch interval时间内处理完毕，以免造成数据堆积。

二、Dstream批次时间

根据spark streaming计算的性质，在一定的集群资源限制下，批次间隔的值会极大地影响系统的数据处理能力

，对于特定的数据速率，一个系统可能能够在批次间隔为2秒时跟上数据接收速度，但如果把批次间隔改为500毫秒系统可能就处理不过来了。所以，批次间隔需要谨慎设置，以确保生产系统能够处理得过来

要找出适合的批次间隔，你可以从一个比较保守的批次间隔值（如5~10秒）开始测试。要验证系统是否能跟上当前的数据接收速率，你可能需要检查一下端到端的批次处理延迟（可以看看Spark驱动器log4j日志中的Total delay，也可以用StreamingListener接口来检测）。如果这个延迟能保持和批次间隔差不多，那么系统基本就是稳定的。否则，如果这个延迟持久在增长，也就是说系统跟不上数据接收速度，那也就意味着系统不稳定。一旦系统文档下来后，你就可以尝试提高数据接收速度，或者减少批次间隔值。不过需要注意，瞬间的延迟增长可以只是暂时的，只要这个延迟后续会自动降下来就没有问题（如：降到小于批次间隔值）

三、执行过程

1.初始化StreamingContext对象，在该对象启动过程中实例化DStreamGraph和JobScheduler，其中DStreamGraph用于存放DStream以及DStream之间的依赖关系等信息，而JobScheduler中包括ReceiverTracker和JobGenerator。其中ReceiverTracker为Driver端流数据接收器（Receiver）的管理者，JobGenerator为批处理作业生成器。在ReceiverTracker启动过程中，根据流数据接收器分发策略通知对应的Executor中的流数据接收管理器（ReciverSupervisor）启动，再由ReciverSupervisor启动流数据接收器。
2.当流数据接收器Receiver启动后，持续不断地接收实时流数据，根据传过来数据的大小进行判断，如果数据量很小，则攒多条数据成一块，然后再进行块存储，如果数据量大，则直接进行块存储。对于这些数据Receiver直接交给ReciverSupervisor，由其进行数据转储操作。块存储根据设置是否预写日志分为两种，一种是使用非预写日志BlockManagerBasedBlockHandler方法直接写到Worker的内存或磁盘中，另一种是进行预写日志WriteAheadLogBasedBlockHandler方法，即在预写日志同时把数据写入到Worker的内存或磁盘中。数据存储完毕后，ReciverSupervisor会把数据存储的元信息上报给ReceiverTracker，ReceiverTracker再把这些信息转发给ReceivedBlockTracker，由它负责管理收到数据块的元信息。
3.在StreamingContext的JobGenerator中维护一个定时器，该定时器在批处理时间到来时会进行生成作业的操作。在该操作中进行：
i.通知ReceiverTracker将接收到的数据进行提交，在提交时采用synchronized 关键字进行处理，保证每条数据被划入一个且只被划入一个批中；
ii.要求DStreamGraph根据DStream依赖关系生成作业序列Seq[Job]；
iii.从第一步中ReceiverTracker获取本批次数据的元数据；
iv.把批处理时间time，作业序列Seq[Job]和本批次数据的元数据包装为JobSet，调用JobScheduler.submitJobSet(JobSet) 提交给 JobScheduler，JobScheduler将把这些作业发送给Spark核心进行处理，由于该执行为异步，因此本步将非常快；
v.只要提交结束（不管作业是否执行情况），Spark Streaming对整个系统做一个检查点（Checkpoint）。
4.在Spark核心的作业对数据进行处理，处理完毕后输出到外部系统，如数据库或文件系统，输出的数据可以被外部系统所使用。由于实时流数据的数据源源不断流入，Spark会周而复始地进行数据处理，相应也会持续不断地输出结果。



四、高容错

​	热备：



五、窗口计算



六、DataFrame和RDD的关系

​	DataSet:

​		数据集是数据的分布式集合。Dataset是Spark 1.6中添加的新接口，它提供了RDDs(强类型、使用强大lambda函数的能力)和Spark SQL s优化执行引擎的优点。可以从JVM对象构造数据集，然后使用功能转换(映射、平面映射、过滤器等)进行操作。数据集API在Scala和Java中可用。Python不支持数据集API。但是由于Python的动态性，数据集API的许多优点已经可用(例如，您可以按行名称自然地访问行。columnname的字段)。R的情况类似。

​	数据集类似于RDDs，但是，它们不使用Java序列化或Kryo，而是使用专门的编码器序列化对象，以便通过网络进行处理或传输。虽然编码器和标准序列化都负责将对象转换成字节，但编码器是动态生成的代码，使用的格式允许Spark执行许多操作，如过滤、排序和哈希，而无需将字节反序列化回对象。

​	DataFrame：

​	DataFrame是组织到命名列中的数据集。它在概念上等价于关系数据库中的表或R/Python中的数据框架，但是在底层有更丰富的优化。DataFrames可以从许多数据源构建，例如:结构化数据文件、Hive中的表、外部数据库或现有的rdd。DataFrame API在Scala、Java、Python和r中都可以使用。在Scala和Java中，数据aframe由行数据集表示。在Scala API中，DataFrame只是数据集[Row]的类型别名。而在Java API中，用户需要使用Dataset<Row>来表示一个DataFrame。



七、广播变量

​	1.修改在哪里？

​	2.如果不修改，是否可以直接替代一个只读变量传递到excutor

